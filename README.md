# Multiview Xâ€‘Ray 3D Hazardous Object Detection

<p align="center">
  <img src="assets/teaser.png" width="100%" alt="teaser">
</p>

Robust endâ€‘toâ€‘end pipeline for reconstructing **3D bounding boxes of hazardous objects from multiview Xâ€‘ray images**. The system couples a highâ€‘recall YOLOâ€‘based 2D detector with a **visualâ€‘hull** intersection and a **fanâ€‘beam camera geometry** that treats **X as perspective** and **Y as independent**, enabling stable height reasoning and efficient 3D recovery even when views are cluttered or partially occluded.

---

## ğŸŒŸ News
- **2025â€‘09â€‘04** â€” [Technical Report](report.pdf) and full code is released.
- **2024â€‘12â€‘30** â€” Released 2D training & standalone inference under `detector2d/`.

---

## ğŸ” Highlights
- **Fourâ€‘stage pipeline:** (1) 2D detection â†’ (2) visualâ€‘hull voxel constraints â†’ (3) consistency filtering across views â†’ (4) 3D bounding box recovery.
- **Fanâ€‘beam with orthographicâ€‘Y:** keep perspective in **X** while approximating **Y** as orthographic, simplifying reprojection and height estimation.
- **Works with â‰¥2 views** (typical: 9 views) and requires prior calibration.
- **Evaluation** via 3Dâ†’2D reprojection and IoU vs. detected 2D boxes.
- Modular design: training code for the detector is isolated under `detector2d/`, while the pipeline entry points remain at the repo root.

---

## ğŸ—‚ Repository Structure

```text
.
â”œâ”€ calibration.py          # Fanâ€‘beam calibration from beads â†’ npy params
â”œâ”€ detector.py             # Pipeline 2D inference â†’ JSON + overlays
â”œâ”€ visual_hull.py          # Visualâ€‘hull + 3D bbox recovery + optional viewer
â”œâ”€ eval.py                 # Reprojects 3D â†’ 2D and reports IoU stats
â”œâ”€ run.py                  # Convenience runner: detection â†’ visual hull
â”œâ”€ vh_utils/               # Visualâ€‘hull utilities (IO, geometry, hull ops, viz)
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ io.py
â”‚  â”œâ”€ geometry.py
â”‚  â”œâ”€ hull.py
â”‚  â””â”€ viz.py
â”œâ”€ detector2d/             # 2D training/eval/standalone inference (Ultralytics)
â”‚  â”œâ”€ train.py
â”‚  â”œâ”€ eval.py
â”‚  â”œâ”€ inference.py
â”‚  â””â”€ configs/config.yaml
â”œâ”€ models/
â”‚  â”œâ”€ best.pt              # Inference checkpoint used by pipeline detector
â”‚  â””â”€ yolo11x.pt           # Optional pretrain for training
â”œâ”€ data/
â”‚  â”œâ”€ calibration/
â”‚  â”‚  â”œâ”€ calibration_results.npy
â”‚  â”‚  â””â”€ 0/{2d.npy,3d.npy}
â”‚  â”œâ”€ image/0/0.png        # Raw multiview images per case ID
â”‚  â”œâ”€ voxel/0/0_512x512x619.npy
â”‚  â”œâ”€ bbox2d/              # detector.py outputs JSON + overlays
â”‚  â”œâ”€ bbox3d/              # visual_hull.py outputs 3D boxes (per class)
â”‚  â””â”€ eval_results/        # eval.py reports
â”œâ”€ assets/                 # README images (teaser, dataset, eval)
â””â”€ requirements.txt
```

---

## âš™ï¸ Installation

```bash
# (Optional) Create a clean env
conda create -n xray3d python=3.10 -y
conda activate xray3d

# Install deps
pip install -r requirements.txt
```

`requirements.txt` includes: `numpy`, `opencv-python`, `open3d`, `ultralytics`, `torch`, `torchvision`, `gdown`.

---

## ğŸ“¦ Data Layouts

### Multiview 3D pipeline (per case ID)

```
data/
  calibration/
    calibration_results.npy
    0/ {2d.npy, 3d.npy}     # beads for calibration (you can include multiple sets)
  image/<ID>/  *.png        # multiview Xâ€‘ray images for a case
  voxel/<ID>/  *_WxHxD.npy  # raw 3D points/voxel npy (visualization + scale)
```

### 2D training dataset (YOLO format)

```
detector2d/data/
  images/{train,val,test}/*.png
  labels/{train,val,test}/*.txt   # YOLO xywh normalized
```

---

## ğŸ§  Method Overview

### Fanâ€‘beam camera with orthographicâ€‘Y
We model each view with perâ€‘camera parameters `[Tx, Tz, Î¸, DSD]` and a fanâ€‘beam projection. Due to the imaging geometry, vertical scale (Y) is approximately viewâ€‘invariant; we therefore keep **perspective** along **X** but treat **Y** as **orthographic**. This simplifies reprojection math and stabilizes height estimation.

### Visual hull on the ground plane (Xâ€“Z)
Given 2D detections across views, we project horizontal bbox constraints into a common Xâ€“Z grid and intersect them to obtain a feasible groundâ€‘plane footprint. We summarize the footprint by a **minâ€‘area rectangle** and lift it into 3D using a global Yâ€‘extent derived from the perâ€‘view bbox vertical bounds (orthographicâ€‘Y mapping), with optional margin.

### Multiâ€‘view consistency
We group detections across views via:
- **Height consistency:** similar top/bottom Y across views.
- **Rayâ€‘intersection consistency:** backâ€‘project top bbox corners and require intersecting rays in Xâ€“Z for compatibility.

Only candidates supported by â‰¥ *k* views (default `k=4`) proceed to the hull step.

### Evaluation by reprojection
Each recovered 3D box is reprojected into each view using the mixed projection (perspectiveâ€‘X, orthographicâ€‘Y); we compute IoU with available 2D detections and summarize perâ€‘view and global stats.

---

## ğŸš€ Quickstart (Pipeline)

### 1) Calibration

<p align="center">
  <img src="assets/beads.png" width="35%" alt="Beads phantom"><br/>
  Beads phantom used for calibration
</p>

<p align="center">
  <img src="assets/calibration_process.gif" width="55%" alt="Calibration process"><br/>
  Camera calibration process
</p>

Optimizes `[Tx, Tz, Î¸]` per camera and optionally `DSD` from 3Dâ€“2D bead correspondences.

```bash
python calibration.py \
  --input data/calibration \
  --DSD 1100 \
  --iter 10000 \
  --output data/calibration/calibration_results.npy \
  --scheduler none
```

**Tips**
- Good defaults: `iter=10000`, `lr=0.1`, `DSD=1100`.
- Optional flags: `--optim_dsd`, `--optim_beads`, `--scheduler steplr|exponential`.

### 2) 2D Detection (pipeline inference)
Low confidence (e.g., `0.1`) is recommended to favor recall; geometric checks will prune outliers.

<p align="center">
  <img src="assets/detector2d.png" width="35%" alt="detector2d"><br/>
  2D detection results on x-ray images
</p>


```bash
python detector.py \
  --input data/image/0 \
  --model models/best.pt \
  --conf-thres 0.1 \
  --output-dir data/bbox2d
# â†’ writes JSON and overlays to data/bbox2d/0/
```

### 3) Visual Hull + 3D Boxes
Builds the groundâ€‘plane hull and lifts into 3D using a global Y extent.

<p align="center">
  <img src="assets/visualhull_process.gif" width="55%" alt="detector2d"><br/>
  Visualhull for obtaining ground-plane hull
</p>
<p align="center">
  <img src="assets/ransac_process.gif" width="75%" alt="detector2d"><br/>
  RANSAC for filtering outliers
</p>

```bash
python visual_hull.py \
  --name 0 \
  --calibration_path data/calibration/calibration_results.npy \
  --min_detection 4 \
  --margin 0.0 0.5 \
  --visualization False
# â†’ writes perâ€‘class 3D bboxes to data/bbox3d/0/
```

### 4) Evaluation (3Dâ†’2D IoU)
<p align="center">
<p align="center">
  <img src="assets/evaluation.png" width="45%" alt="Evaluation: 3Dâ†’2D projection vs GT"><br/>
  <em>
    Projection of predicted 3D bounding boxes into each view vs. detector 2D ground truth.
    ğŸŸ© <b>Green</b> = Ground Truth, ğŸŸª <b>Purple</b> = 3D Projection.
  </em>
</p>

```bash
python eval.py --id 0 --save_images
# â†’ saves overlays under data/eval_results/0/ and summary JSON in data/eval_results/
```

### Oneâ€‘Command Demo
```bash
python run.py --id 0 --vis
# runs detector.py â†’ visual_hull.py (with optional viewer)
```

---

## ğŸ§ª 2D Detector (Training / Eval / Standalone Inference)
The training code and configs are isolated under `detector2d/` so you can iterate on the detector independently of the 3D pipeline.

<p align="center">
  <img src="assets/detector2d_data.png" width="55%" alt="detector2d"><br/>
  2D X-Ray Dataset
</p>

### Dataset
- **Format**: YOLO (normalized `cx cy w h` per line).
- **Splits**:
  - **Train**: 212,119 images
  - **Val**: 24,104 images
  - **Test**: 24,236 images
- **Classes**: 66 classes (the config lists `nc: 67` because it includes a `"None"` placeholder at index 0).
- **Config**: `detector2d/configs/config.yaml` defines split paths and the class taxonomy.

**Due to license restrictions, we cannot release our internal dataset.**  
However, you can use an open-source alternative such as the AI-Hub X-ray security dataset:
- **AI-Hub:** [Link](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71442)

> Tip: If the dataset does not already follow YOLO format, convert labels to normalized `cx cy w h` per image to train with `detector2d/configs/config.yaml`.


### Train
```bash
python detector2d/train.py
# uses detector2d/configs/config.yaml
# multiâ€‘GPU supported via Ultralytics; adjust device list and batch size
```

### Evaluate detector

<p align="center">
  <img src="assets/detector2d_eval.png" width="55%" alt="detector2d"><br/>
  Evaluation results
</p>

```bash
python detector2d/eval.py
```

The evaluation results for the provided pretrained checkpoints are as follows: 
| Class                         | Images | Instances | Box(P) | Box(R) | mAP50 | mAP50-95 |
|-------------------------------|--------|-----------|--------|--------|-------|----------|
| all                           | 24104  | 31292     | 0.975  | 0.932  | 0.953 | 0.871    |


### Standalone inference

<p align="center">
  <img src="assets/detector2d_inference.png" width="55%" alt="detector2d"><br/>
  Model inference results
</p>

```bash
python detector2d/inference.py \
  --input detector2d/data/images/test \
  --ckpt runs/train/yolo11x/weights/best.pt \
  --output out/infer \
  --save_annot
```

> The pipelineâ€™s root `detector.py` remains unchanged and writes JSON/overlays to `data/bbox2d/<ID>` as expected by the 3D step.

---

## ğŸ”§ Configuration & Parameters

- **Calibration**
  - `DSD` (mm): nominal 1100 (tune per scanner).
  - `iter`: 10k typical; Adam with `lr=0.1`; optional LR schedulers.
- **Detector (pipeline inference)**
  - `--conf-thres`: default 0.1 for high recall.
- **Visual Hull**
  - `--min_detection`: default 4 (required number of supporting views).
  - `--margin`: `(x_margin, y_margin)` as proportions; default `(0.0, 0.5)`.
- **Evaluation**
  - Reprojection uses perspectiveâ€‘X and orthographicâ€‘Y to form 2D bboxes.

---

## ğŸ§° Troubleshooting

- **No visual hull found**
  - Check calibration alignment and that image/JSON view indices match.
  - Relax `--margin` or detector `--conf-thres` to increase recall.
- **Inconsistent scale**
  - Ensure voxel NPY filename encodes dimensions like `*_512x512x619.npy`.
  - The pipeline rescales image height to voxel height internally.
- **Sparse views**
  - Use fewer required views (`--min_detection 3`) and increase `y_margin`.

---

## ğŸ“œ License

The model is licensed under the [Apache 2.0 license](LICENSE).

## ğŸ¤— Contributing

See [contributing](CONTRIBUTING.md) and the [code of conduct](CODE_OF_CONDUCT.md).

## ğŸ™Œ Acknowledgements
The 2D Detection Model was built apon the [Ultralytics](https://www.ultralytics.com/) YOLO Model. For deeper background and ablations, please refer to the accompanying [technical report](report.pdf).

---

## ğŸ“š Citation
If this project or its data/models are useful in your research, please consider citing:

```bibtex
@misc{3dxray2024,
  author = {SECERN AI},
  title  = {Multiview Xâ€‘Ray 3D Hazardous Object Detection},
  year   = {2024},
  url    = {https://github.com/comb0601/3D_Xray}
}
```
